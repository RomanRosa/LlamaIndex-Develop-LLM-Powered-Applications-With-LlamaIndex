{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import download_loader\n",
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SimpleWebPageReader class\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main(url: str) -> None:\n",
    "    # Initialize the SimpleWebPageReader and load data from the URL\n",
    "    reader = SimpleWebPageReader(html_to_text=True)\n",
    "    documents = reader.load_data(urls=[url])\n",
    "    index= VectorStoreIndex.from_documents(documents=documents)\n",
    "    query_engine= index.as_query_engine()\n",
    "    response= query_engine.query(\"What is llamaindex?\")\n",
    "    print(response)\n",
    "    for document in documents:\n",
    "        print(document) # Adjust this depending on how you want to display the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a data framework for LLM-based applications. It allows users to ingest, structure, and access private or domain-specific data. LlamaIndex provides tools such as data connectors, data indexes, engines, data agents, and application integrations to facilitate natural language access to data. It overcomes the limitations of fine-tuning LLMs by using a Retrieval-Augmented Generation (RAG) approach, where information is retrieved from data sources and added to the question as context before asking the LLM to generate an answer. LlamaIndex is available in Python and Typescript.\n",
      "Doc ID: ff482f02-21e6-4584-a0b1-2d8982e8997b\n",
      "Text: Contents Menu Expand Light mode Dark mode Auto light/dark mode\n",
      "Hide navigation sidebar  Hide table of contents sidebar  Toggle site\n",
      "navigation sidebar  __  LlamaIndex ðŸ¦™ 0.9.43  Toggle Light / Dark /\n",
      "Auto color theme  Toggle table of contents sidebar  __  LlamaIndex ðŸ¦™\n",
      "0.9.43  Getting Started    * [Installation and\n",
      "Setup](getting_started/installa...\n"
     ]
    }
   ],
   "source": [
    "# Execute the main function when running the script\n",
    "if __name__ == '__main__':\n",
    "    # Import necessary modules\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "\n",
    "    # Call the main function with the specified URL\n",
    "    main(url=\"https://docs.llamaindex.ai/en/stable/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
